{\bf Setup: $n$-gram language models and uniform-cost search}

Our algorithm will base its segmentation and insertion decisions on the cost of
processed text according to a {\em language model}. A language model is some
function of the processed text that captures its fluency.

A very common language model in NLP is an $n$-gram sequence model. This is a
function that, given $n$ consecutive words, provides a cost based on the
negative log likelihood that the $n$-th word appears just after the first $n-1$
words.\footnote{This model works under the assumption that text roughly
satisfies the \href{https://en.wikipedia.org/wiki/Markov_property} Markov
 property.}

The cost will always be positive, and lower costs indicate better fluency.
\footnote{Modulo edge cases, the $n$-gram model score in this assignment is
given by $\ell(w_1, \ldots, w_n) = -\log(p(w_n \mid w_1, \ldots, w_{n-1}))$.
Here, $p(\cdot)$ is an estimate of the conditional probability distribution over
words given the sequence of previous $n-1$ words.  This estimate is gathered
from frequency counts taken by reading Leo Tolstoy's {\em War and Peace} and
William Shakespeare's {\em Romeo and Juliet}.}

As a simple example: In a case where $n=2$ and $c$ is our $n$-gram cost
function, $c(${\sf big}, {\sf fish}$)$ would be low, but $c(${\sf fish},
{\sf fish}$)$ would be fairly high.

Furthermore, these costs are additive: For a unigram model $u$ ($n = 1$), the
cost assigned to $[w_1, w_2, w_3, w_4]$ is
\[
u(w_1) + u(w_2) + u(w_3) + u(w_4).
\]

Similarly, for a bigram model $b$ ($n = 2$), the cost is
\[
b(w_0, w_1) +
b(w_1, w_2) +
b(w_2, w_3) +
b(w_3, w_4),
\]

where $w_0$ is {\tt -BEGIN-}, a special token that denotes the beginning of the
sentence.

We have estimated $u$ and $b$ based on the statistics of $n$-grams in text. Note
that any words not in the corpus are automatically assigned a high cost, so you
do not have to worry about that part.

A note on low-level efficiency and expectations: This assignment was designed
considering input sequences of length no greater than roughly 200, where these
sequences can be sequences of characters or of list items, depending on the
task.  Of course, it's great if programs can tractably manage larger inputs, but
it's okay if such inputs can lead to inefficiency due to overwhelming state
space growth.
